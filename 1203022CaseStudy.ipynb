{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP/Z/vbV5YYzXN+lGqLjdj5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import models\n","from torch.utils.data import DataLoader\n","\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","alex_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","batch_size=32\n","trainset=torchvision.datasets.CIFAR10(root='./data',train=True,download=True,transform=alex_transform)\n","trainloader=DataLoader(trainset, batch_size=batch_size,shuffle=True,num_workers=2)\n","testset=torchvision.datasets.CIFAR10(root='./data',train=False,download=True,transform=alex_transform)\n","testloader=DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","criterion=nn.CrossEntropyLoss()\n","\n","def train_epochs(net,optimizer,title):\n","    print(f'{title}')\n","    net.train()\n","    for epoch in range(3):\n","        running_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            inputs, labels = data\n","            inputs=inputs.to(device)\n","            labels=labels.to(device)\n","            optimizer.zero_grad()\n","            outputs=net(inputs)\n","            loss=criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss+=loss.item()\n","        avg_loss=running_loss/len(trainloader)\n","        print(f'Epoch {epoch + 1} -> Loss: {avg_loss:.4f}')\n","def test_accuracy(net):\n","    net.eval()\n","    correct=0\n","    total=0\n","    with torch.no_grad():\n","        for data in testloader:\n","            images,labels=data\n","            images,labels=images.to(device),labels.to(device)\n","            outputs=net(images)\n","            _,predicted=torch.max(outputs.data,1)\n","            total+=labels.size(0)\n","            correct+=(predicted==labels).sum().item()\n","    acc =100.0*correct/total\n","    print(f'Test Accuracy: {acc:.2f}%')\n","    return acc\n","\n","alex_finetune=models.alexnet(weights='IMAGENET1K_V1')\n","alex_finetune.classifier[6]=nn.Linear(4096, 10)\n","alex_finetune=alex_finetune.to(device)\n","optimizer_alex_ft=optim.SGD(alex_finetune.parameters(),lr=0.001,momentum=0.9)\n","train_epochs(alex_finetune, optimizer_alex_ft,'AlexNet | Finetuning')\n","acc_alex_ft=test_accuracy(alex_finetune)\n","\n","alex_fixed = models.alexnet(weights='IMAGENET1K_V1')\n","alex_fixed.classifier[6] = nn.Linear(4096, 10)\n","for p in alex_fixed.parameters():\n","    p.requires_grad=False\n","for p in alex_fixed.classifier[6].parameters():\n","    p.requires_grad=True\n","alex_fixed=alex_fixed.to(device)\n","optimizer_alex_fx=optim.SGD(alex_fixed.classifier[6].parameters(),lr=0.001,momentum=0.9)\n","train_epochs(alex_fixed,optimizer_alex_fx,'AlexNet | Feature Extractor')\n","acc_alex_fx=test_accuracy(alex_fixed)\n","\n","vgg_finetune=models.vgg16(weights='IMAGENET1K_V1')\n","vgg_finetune.classifier[6]=nn.Linear(4096, 10)\n","vgg_finetune=vgg_finetune.to(device)\n","optimizer_vgg_ft=optim.SGD(vgg_finetune.parameters(),lr=0.001,momentum=0.9)\n","train_epochs(vgg_finetune,optimizer_vgg_ft,'VGG16 | Finetuning')\n","acc_vgg_ft=test_accuracy(vgg_finetune)\n","\n","vgg_fixed=models.vgg16(weights='IMAGENET1K_V1')\n","vgg_fixed.classifier[6]=nn.Linear(4096, 10)\n","for p in vgg_fixed.parameters():\n","    p.requires_grad=False\n","for p in vgg_fixed.classifier[6].parameters():\n","    p.requires_grad=True\n","vgg_fixed=vgg_fixed.to(device)\n","optimizer_vgg_fx=optim.SGD(vgg_fixed.classifier[6].parameters(),lr=0.001,momentum=0.9)\n","train_epochs(vgg_fixed, optimizer_vgg_fx,'VGG16 | Feature Extractor')\n","acc_vgg_fx=test_accuracy(vgg_fixed)\n","\n","print('Summary:')\n","print('AlexNet   (Finetune):  ',f'{acc_alex_ft:.2f}%')\n","print('AlexNet   (Fixed):     ',f'{acc_alex_fx:.2f}%')\n","print('VGG16     (Finetune):  ',f'{acc_vgg_ft:.2f}%')\n","print('VGG16     (Fixed):     ',f'{acc_vgg_fx:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xnkslHRlqpZK","executionInfo":{"status":"ok","timestamp":1755184673876,"user_tz":-180,"elapsed":3801562,"user":{"displayName":"Mohammed Salem","userId":"15899953639954507769"}},"outputId":"623a84d0-e7ca-464f-8316-5d50794fe402"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AlexNet | Finetuning\n","Epoch 1 -> Loss: 0.6344\n","Epoch 2 -> Loss: 0.3991\n","Epoch 3 -> Loss: 0.3120\n","Test Accuracy: 89.17%\n","AlexNet | Feature Extractor\n","Epoch 1 -> Loss: 0.7910\n","Epoch 2 -> Loss: 0.6848\n","Epoch 3 -> Loss: 0.6561\n","Test Accuracy: 81.32%\n","VGG16 | Finetuning\n","Epoch 1 -> Loss: 0.4812\n","Epoch 2 -> Loss: 0.2438\n","Epoch 3 -> Loss: 0.1575\n","Test Accuracy: 92.23%\n","VGG16 | Feature Extractor\n","Epoch 1 -> Loss: 0.7395\n","Epoch 2 -> Loss: 0.6367\n","Epoch 3 -> Loss: 0.6146\n","Test Accuracy: 81.96%\n","Summary:\n","AlexNet   (Finetune):   89.17%\n","AlexNet   (Fixed):      81.32%\n","VGG16     (Finetune):   92.23%\n","VGG16     (Fixed):      81.96%\n"]}]},{"cell_type":"markdown","source":["From the four runs on CIFAR-10 (3 epochs and batch size 32)the pattern is clear finetuning beats fixed feature extraction for both models and VGG16 > AlexNet when we allow all weights to adapt With finetuning VGG16 reached 92.23% while AlexNet achieved 89.17% in contrast freezing the backbone and training only the last fully connected layer capped performance around 82% for both (81.96% VGG16,81.32% AlexNet). The loss curves explain why finetuned models showed strong steady decreases each epoch (VGG16 FT from 0.4812→0.2432→0.1575, AlexNet FT 0.6344 →0.3991→0.3120) meaning the entire feature hierarchy was being reshaped for CIFAR-10 When used as fixed feature extractors the backbones stayed ImageNet-biased so the classifier could only make limited improvements (loss dropped much less: VGG16 FX 0.7395→0.6367→0.6146 AlexNet FX 0.7919→0.6848→0.6561). The finetuning advantage is expected because CIFAR-10 images 10 classes smaller objects different textures differ from ImageNet so allowing convolutional layers to update helps align low and midlevel filters to this dataset VGG16’s deeper architecture likely explains its edge over AlexNet when finetuned (more capacity to model fine details) while in the frozen setting both sit near the same ceiling since only the last layer learns. Overall these results suggest that for CIFAR10, updating the whole network is worth it and VGG16 yields the best accuracy when finetuned with all models showing healthy monotonic loss reductions across the three epochs."],"metadata":{"id":"459QYPCbK9Gc"}}]}